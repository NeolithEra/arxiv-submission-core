"""
Submission Event Consumer
=========================

The submission event consumer is responsible for monitoring submission event,
evaluating them against pre-defined rules, and triggering processes to be
carried out by the :mod:`agent.worker`.

The consumer is implemented using :mod:`arxiv.integration.kinesis`, and
conumes events in the ``SubmissionEvents`` Kinesis stream. Events may
be generated by submission user interfaces, APIs, and backend components
that leverage the ``arxiv.submission`` core package. As events are consumed,
they are evaluated against a set of registered :class:`.Rule` instances, which
map event types and conditions to :class:`.ProcessType` classes.

Consequent processes are not run in the consumer application, which is run as a
single thread. The consumer tries to move on as quickly as possible, so it uses
the :class:`.AsyncProcessRunner` to dispatch processes for parallel execution
by the worker.

The event lifecycle from the perspective of the consumer looks like this:

1. A command/event is generated by a submission service, using the
   :mod:`arxiv.submission` package. The event is stored in the database, and
   propagated via the ``SubmissionEvents`` Kinesis stream. The Kinesis payload
   includes the event itself, and the state of the submission both before and
   after the event was applied.
2. The event is consumed by the agent via the ``SubmissionEvents`` Kinesis
   stream.
3. The agent evaluates the event against registered :class:`.Rule` instances,
   using :func:`.rules.evaluate`. A :class:`.Rule` maps a condition (the event
   type and event/submission properties) to a :class:`.Process`.
4. The agent dispatches any triggered :class:`.Proccess` instances to the
   :mod:`agent.worker` using the :class:`.AsyncProcessRunner`.


Components
----------

.. _figure-submission-agent-consumer-components:

.. figure:: _static/diagrams/submission-agent-consumer-components.png
   :width: 600px

   Main components of the event consumer.


The :class:`SubmissionEventConsumer` defines how records from the
``SubmissionEvents`` stream are handled. This is the primary point of control
in the agent. As events are received, it relies on :mod:`agent.rules` to
determine what processes to carry out, and then dispatches those processes
to the :mod:`agent.worker` using the :class:`.AsyncProcessRunner`.

The :class:`SubmissionEventConsumer` relies on the
:class:`.DatabaseCheckpointManager` to keep track of its progress in the
``SubmissionEvents`` stream.

The :mod:`agent.services.database` integration module provides access to the
agent database. Specifically, it supports creating and loading checkpoints,
and storing information about process-relevant events.

Processes are defined in :mod:`agent.process`. Each process is a subclass of
:class:`.Process`, and may have one or more steps.

Rules are defined in :mod:`agent.rules`. Each rule is an instantiation of
:class:`.Rule` in the root of that module. It relies on the event types
defined in :mod:`arxiv.submission.domain.events`, and the processes defined in
:mod:`agent.process`.

:mod:`agent.runner` provides tools for running :class:`.Process` instances. The
base :class:`.ProcessRunner` carries out the process in a single thread, which
may be useful for testing purposes. The runner used in production is the
:class:`.AsyncProcessRunner`, which manages registration and dispatching of
asynchronous tasks carried out by the :mod:`agent.worker`.


"""

import json
import os
import time
from typing import List, Any, Optional, Dict

from flask import Flask
from retry import retry

from botocore.exceptions import WaiterError, NoCredentialsError, \
    PartialCredentialsError, BotoCoreError, ClientError

from arxiv.base import logging
from arxiv.integration.kinesis import consumer
from arxiv.submission.serializer import loads
from arxiv.submission.domain.submission import Submission
from arxiv.submission.domain.event import Event, AddProcessStatus

from . import rules
from .services import database
from .factory import create_app
from .domain import Trigger
from .runner import AsyncProcessRunner
from .process import Process

logger = logging.getLogger(__name__)
logger.propagate = False


class SubmissionEventConsumer(consumer.BaseConsumer):
    """
    Consumes submission events, and dispatches processes based on rules.

    .. todo:
       ARXIVNG-2041 Implement throttling control in base Kinesis integration


    .. todo:
       ARXIVNG-2042 Retry should be configurable in base Kinesis controller
       without overriding methods

    """

    def process_record(self, record: dict) -> None:
        """
        Evaluate an event against registered rules.

        Parameters
        ----------
        data : bytes
        partition_key : bytes
        sequence_number : int
        sub_sequence_number : int

        """
        logger.info(f'Processing record %s', record["SequenceNumber"])
        try:
            data = loads(record['Data'].decode('utf-8'))
        except json.decoder.JSONDecodeError as exc:
            logger.error("Error (%s) while deserializing from data %s",
                         exc, record['Data'])
            raise exc
        event, before, after = data['event'], data['before'], data['after']

        # We want to keep track of process-related events, so that we can
        # reconstruct what happened if necessary.
        if type(event) is AddProcessStatus:
            self._store_event(event)

        # rules.evaluate() will yield any processes and corresponding
        # configuration paramters that are triggered by matching rules.
        logger.debug('Evaluating event %s', event.event_id)
        for process, params in rules.evaluate(event, before, after):
            self._dispatch_process(process, params, event, before, after)
        logger.debug('Done processing record %s', record["SequenceNumber"])

    @retry(backoff=2, jitter=(0, 1), logger=logger)
    def _store_event(self, event: AddProcessStatus) -> None:
        logger.debug('Storing event %s', event)
        database.store_event(event)
        logger.debug('..stored.')

    @retry(backoff=2, jitter=(0, 1), logger=logger)
    def _dispatch_process(self, process: Process, params: Dict[str, Any],
                          event: Event, before: Submission,
                          after: Submission) -> None:
        trigger = Trigger(event=event, before=before, after=after,
                          actor=event.creator, params=params)

        logger.debug('starting process %s', process.name)
        runner = AsyncProcessRunner(process)
        runner.run(trigger)
        logger.info('Event %s on submission %s caused %s with params %s',
                    event.event_id, event.submission_id, process.name,
                    params)

    def wait_for_stream(self) -> None:
        """
        Wait for the stream to become available.

        If the stream becomes available, returns ``None``. Otherwise, raises
        a :class:`.StreamNotAvailable` exception.

        Raises
        ------
        :class:`.StreamNotAvailable`
            Raised when the stream could not be reached.

        """
        waiter = self.client.get_waiter('stream_exists')
        try:
            logger.error(f'Waiting for stream {self.stream_name}')
            waiter.wait(
                StreamName=self.stream_name,
                Limit=1,
                ExclusiveStartShardId=self.shard_id
            )
        except WaiterError as e:
            logger.error('Failed to get stream while waiting')
            raise consumer.exceptions.StreamNotAvailable('Could not connect to stream') from e
        except (PartialCredentialsError, NoCredentialsError) as e:
            logger.error('Credentials missing or incomplete: %s', e.msg)
            raise consumer.exceptions.ConfigurationError('Credentials missing') from e
        logger.debug('Done waiting')

    def go(self) -> None:
        logger.debug('Go!')
        super(SubmissionEventConsumer, self).go()


class DatabaseCheckpointManager:
    """
    Provides database-backed loading and updating of consumer checkpoints.
    """

    def __init__(self, shard_id: str) -> None:
        """Get the last checkpoint."""
        self.shard_id = shard_id
        self.position = database.get_latest_position(self.shard_id)

    def checkpoint(self, position: str) -> None:
        """Checkpoint at ``position``."""
        try:
            database.store_position(position, self.shard_id)
            self.position = position
        except Exception as e:
            raise consumer.CheckpointError('Could not checkpoint') from e


def process_stream(app: Flask, duration: Optional[int] = None) -> None:
    """
    Configure and run the record processor.

    Parameters
    ----------
    duration : int
        Time (in seconds) to run record processing. If None (default), will
        run "forever".

    """
    # We use the Flask application instance for configuration, and to manage
    # integrations with metadata service, search index.
    checkpointer = DatabaseCheckpointManager(app.config['KINESIS_SHARD_ID'])
    consumer.process_stream(SubmissionEventConsumer, app.config,
                            checkpointmanager=checkpointer, duration=duration)


def start_agent() -> None:
    """Start the record processor."""
    app = create_app()
    with app.app_context():
        database.await_connection()
        if not database.tables_exist():
            database.create_all()
        process_stream(app)


if __name__ == '__main__':
    start_agent()
